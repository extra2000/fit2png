{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "6514aeeb1147f9b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import io\n",
    "import datetime\n",
    "from os import path, makedirs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "\n",
    "from fit2png.common import SEMICIRCLES_TO_DEGREES\n",
    "from fit2png.utils import read_fit, render_hud, computer_vision_hud"
   ],
   "id": "374e8e99025890af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configurations",
   "id": "83c8b704c72edae1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MAX_TAILS = 30\n",
    "MAX_HR = 172\n",
    "GEOPY_CALL_INTERVAL = 1\n",
    "HUD_WAIT_FOR_GEOPY = True\n",
    "FIT_FILENAME = 'input/20260223214525.fit'\n",
    "VIDEO_PATHS = [\n",
    "    \"/path/to/video.mp4\",\n",
    "]\n",
    "MODEL_BBOX = \"models/yolo26x.pt\"\n",
    "MODEL_SEG = \"models/yolo26x-seg.pt\"\n",
    "\n",
    "fit_id = path.splitext(path.basename(FIT_FILENAME))[0]\n",
    "HUD_OUTDIR = path.join('rendered', fit_id, 'hud')\n",
    "HUD_REDACTED_OUTDIR = path.join('rendered', fit_id, 'hud_redacted')\n",
    "MINIMAP_OUTDIR = path.join('rendered', fit_id, 'minimap')\n",
    "BBOX_OUTDIR = path.join('rendered', fit_id, 'bbox')\n",
    "SEG_OUTDIR = path.join('rendered', fit_id, 'seg')\n",
    "LABEL_OUTDIR = path.join('rendered', fit_id, 'label')\n",
    "AUDIOMETER_OUTDIR = path.join('rendered', fit_id, 'audiometer')\n",
    "\n",
    "GOLDEN_RATIO_CONJUGATE = 0.618033988749895"
   ],
   "id": "4be24556a5e7e10a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parse FIT file",
   "id": "5f0e134690e39521"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = read_fit(FIT_FILENAME)\n",
    "print(data['file_id_mesgs'][0])"
   ],
   "id": "4456e266f3bbba89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HUD redacted",
   "id": "72c6cd5bdb1fb8f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# makedirs(HUD_REDACTED_OUTDIR, exist_ok=True)\n",
    "# render_hud(data, MAX_HR, HUD_REDACTED_OUTDIR, geopy_call_interval=GEOPY_CALL_INTERVAL, enforce_privacy=True)"
   ],
   "id": "1085faf5584c652b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HUD without redacted",
   "id": "fd03cdc7edb7f0cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "makedirs(HUD_OUTDIR, exist_ok=True)\n",
    "render_hud(data, MAX_HR, HUD_OUTDIR, geopy_call_interval=GEOPY_CALL_INTERVAL, wait_for_geopy=HUD_WAIT_FOR_GEOPY, enforce_privacy=False)"
   ],
   "id": "78fba03098dbb542",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Minimap",
   "id": "38d51fa91d767445"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def apply_levels(image, black_point, gamma, white_point):\n",
    "    \"\"\"Applies GIMP-like levels adjustment to an RGB/RGBA image.\"\"\"\n",
    "    # Create a lookup table for the adjustment\n",
    "    lut = []\n",
    "    for i in range(256):\n",
    "        # Clamp and normalize input\n",
    "        v = min(max(i, black_point), white_point)\n",
    "        normalized = (v - black_point) / (white_point - black_point)\n",
    "        # Apply gamma and scale back to 0-255\n",
    "        res = pow(normalized, 1.0 / gamma) * 255\n",
    "        lut.append(int(res))\n",
    "\n",
    "    # Apply to RGB channels only (preserving Alpha if present)\n",
    "    if image.mode == 'RGBA':\n",
    "        r, g, b, a = image.split()\n",
    "        r = r.point(lut)\n",
    "        g = g.point(lut)\n",
    "        b = b.point(lut)\n",
    "        return Image.merge('RGBA', (r, g, b, a))\n",
    "    return image.point(lut)\n",
    "\n",
    "diff_time = 0\n",
    "frame_counter = 0\n",
    "fig, ax = plt.subplots(figsize=(5, 5), frameon=False)\n",
    "\n",
    "makedirs(MINIMAP_OUTDIR, exist_ok=True)\n",
    "\n",
    "for current_coord_index in range(0, len(data['record_mesgs'])):\n",
    "    ax.clear()\n",
    "    ax.set_axis_off()\n",
    "    for i in range(max(0, current_coord_index - MAX_TAILS), current_coord_index + 1):\n",
    "        x = data['record_mesgs'][i]\n",
    "\n",
    "        pos_lat = x.get('position_lat', None)\n",
    "        pos_long = x.get('position_long', None)\n",
    "        if pos_lat is not None and pos_long is not None:\n",
    "            pos_lat = x['position_lat'] * SEMICIRCLES_TO_DEGREES\n",
    "            pos_long = x['position_long'] * SEMICIRCLES_TO_DEGREES\n",
    "\n",
    "            # Lower margin, higher the zoom. Also crops the basemap to reduce bandwidth\n",
    "            margin = 0.001\n",
    "            ax.set_xlim(pos_long - margin, pos_long + margin)\n",
    "            ax.set_ylim(pos_lat - margin, pos_lat + margin)\n",
    "\n",
    "            # Add the basemap but force a LOWER zoom level (e.g., 15 or 16)\n",
    "            # Standard street level is 18. By forcing 16, the labels will appear 4x larger.\n",
    "            cx.add_basemap(ax,\n",
    "                   crs='EPSG:4326',\n",
    "                   source=cx.providers.OpenStreetMap.Mapnik,\n",
    "                   zoom=19, # Lower zoom = More detail\n",
    "                   attribution=\"\")\n",
    "\n",
    "            cx.add_basemap(ax,\n",
    "                   crs='EPSG:4326',\n",
    "                   source=cx.providers.Esri.WorldImagery,\n",
    "                   zoom=19,\n",
    "                   alpha=0.1,\n",
    "                   attribution=\"\")\n",
    "\n",
    "            # Fix copyright (attribution) text color\n",
    "            if ax.texts:\n",
    "                attribution_text = ax.texts[-1]\n",
    "                attribution_text.set_color('black')\n",
    "\n",
    "            if i < current_coord_index:\n",
    "                # Trails\n",
    "                ax.plot(pos_long, pos_lat, 'bo', markersize=15 - (((current_coord_index+1) - i)*0.5), markeredgecolor='white')\n",
    "            else:\n",
    "                # Head\n",
    "                ax.plot(pos_long, pos_lat, 'ro', markersize=15, markeredgecolor='white')\n",
    "\n",
    "    # 1. Convert Figure to PIL Image\n",
    "    buf = io.BytesIO()\n",
    "    # Use bbox_inches='tight', pad_inches=0 to avoid extra white space\n",
    "    fig.savefig(buf, format='png', transparent=True, bbox_inches='tight', pad_inches=0)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf).convert(\"RGBA\")\n",
    "\n",
    "    # Apply GIMP Levels: (low, gamma, high)\n",
    "    img = apply_levels(img, 146, 0.5, 255)\n",
    "\n",
    "    # 2. Create a circular mask\n",
    "    mask = Image.new('L', img.size, 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    # Draw a white circle (255) on the black background (0)\n",
    "    # draw.ellipse((0, 0) + img.size, fill=255)\n",
    "    draw.rectangle((0, 0, img.size[0], img.size[1]), fill=255)\n",
    "\n",
    "    # 3. Apply the mask\n",
    "    output = ImageOps.fit(img, mask.size, centering=(0.5, 0.5))\n",
    "    output.putalpha(mask)\n",
    "\n",
    "    # # 4. Draw a medium thick circle border\n",
    "    # draw_output = ImageDraw.Draw(output)\n",
    "    # border_width = 2\n",
    "    # draw_output.ellipse(\n",
    "    #         (0, 0, output.size[0], output.size[1]),\n",
    "    #         outline=\"black\",\n",
    "    #         width=border_width\n",
    "    #     )\n",
    "\n",
    "    # 5. Save the processed image\n",
    "    output.save(path.join(MINIMAP_OUTDIR, f'{frame_counter:05d}.png'))\n",
    "    frame_counter += 1\n",
    "\n",
    "    current_x = data['record_mesgs'][current_coord_index]\n",
    "    next_x = data['record_mesgs'][current_coord_index+1] if current_coord_index < len(data['record_mesgs']) - 1 else None\n",
    "    current_timestamp = current_x['timestamp'].astimezone(datetime.timezone(datetime.timedelta(hours=8)))\n",
    "    upcoming_time = next_x['timestamp'].astimezone(datetime.timezone(datetime.timedelta(hours=8))) if next_x is not None else None\n",
    "    diff_time = (upcoming_time - current_timestamp).total_seconds() if upcoming_time is not None else 0\n",
    "    if diff_time > 1:\n",
    "        # Pad idle frames for easier Video Editing\n",
    "        for i in range(1, int(diff_time)):\n",
    "            output.save(path.join(MINIMAP_OUTDIR, f'{frame_counter:05d}.png'))\n",
    "            frame_counter += 1\n",
    "\n",
    "    plt.close(fig)"
   ],
   "id": "c34c19aa3071d980",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ComputerVision HUD",
   "id": "b511e7794e2776a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "makedirs(BBOX_OUTDIR, exist_ok=True)\n",
    "makedirs(SEG_OUTDIR, exist_ok=True)\n",
    "makedirs(LABEL_OUTDIR, exist_ok=True)\n",
    "\n",
    "computer_vision_hud(VIDEO_PATHS, MODEL_BBOX, MODEL_SEG, BBOX_OUTDIR, SEG_OUTDIR, LABEL_OUTDIR)"
   ],
   "id": "4d76faa3ca2c7064",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Audio Loudness Meter HUD",
   "id": "f146a83c7d447c4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from matplotlib import font_manager as fm\n",
    "import cv2\n",
    "import av\n",
    "\n",
    "FLOOR_DB = -36.0\n",
    "CEIL_DB = 3.0\n",
    "makedirs(AUDIOMETER_OUTDIR, exist_ok=True)\n",
    "\n",
    "def rms_dbfs(samples: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    samples = np.asarray(samples, dtype=np.float32)\n",
    "    if samples.size == 0:\n",
    "        return float(\"-inf\")\n",
    "    rms = np.sqrt(np.mean(samples * samples) + eps)\n",
    "    return 20.0 * np.log10(rms + eps)\n",
    "\n",
    "def frame_loudness_lr(audio_lr: np.ndarray, i0: int, i1: int) -> tuple[float, float] | tuple[float, None]:\n",
    "    \"\"\"\n",
    "    audio_lr shape: (channels, samples)\n",
    "    Returns (L_dBFS, R_dBFS) when 2+ channels, else (mono_dBFS, None)\n",
    "    \"\"\"\n",
    "    chunk = audio_lr[:, i0:i1]\n",
    "    if chunk.shape[1] == 0:\n",
    "        return float(\"-inf\"), (float(\"-inf\") if audio_lr.shape[0] >= 2 else None)\n",
    "\n",
    "    if audio_lr.shape[0] >= 2:\n",
    "        l = rms_dbfs(chunk[0])\n",
    "        r = rms_dbfs(chunk[1])\n",
    "        return l, r\n",
    "    else:\n",
    "        mono = rms_dbfs(chunk[0])\n",
    "        return mono, None\n",
    "\n",
    "def peak_dbfs(samples: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    \"\"\"\n",
    "    Sample-peak level in dBFS (NOT RMS).\n",
    "    0 dBFS == full scale (abs(sample) == 1.0 for float PCM).\n",
    "    \"\"\"\n",
    "    samples = np.asarray(samples, dtype=np.float32)\n",
    "    if samples.size == 0:\n",
    "        return float(\"-inf\")\n",
    "    peak = float(np.max(np.abs(samples)))\n",
    "    return 20.0 * np.log10(max(peak, eps))\n",
    "\n",
    "def frame_volume_lr_peak(audio_lr: np.ndarray, i0: int, i1: int) -> tuple[float, float] | tuple[float, None]:\n",
    "    \"\"\"\n",
    "    audio_lr shape: (channels, samples)\n",
    "    Returns (L_peak_dBFS, R_peak_dBFS) when 2+ channels, else (mono_peak_dBFS, None)\n",
    "    \"\"\"\n",
    "    chunk = audio_lr[:, i0:i1]\n",
    "    if chunk.shape[1] == 0:\n",
    "        return float(\"-inf\"), (float(\"-inf\") if audio_lr.shape[0] >= 2 else None)\n",
    "\n",
    "    if audio_lr.shape[0] >= 2:\n",
    "        l = peak_dbfs(chunk[0])\n",
    "        r = peak_dbfs(chunk[1])\n",
    "        return l, r\n",
    "    else:\n",
    "        mono = peak_dbfs(chunk[0])\n",
    "        return mono, None\n",
    "\n",
    "frame_idx = 0\n",
    "for video_path in VIDEO_PATHS:\n",
    "    # Video via OpenCV (frames)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if not fps or fps <= 0:\n",
    "        raise RuntimeError(f\"Could not read FPS from video: {video_path}\")\n",
    "    frame_duration = 1.0 / fps\n",
    "\n",
    "    # Audio via PyAV (demux/decode)\n",
    "    container = av.open(video_path)\n",
    "\n",
    "    audio_stream = next((s for s in container.streams if s.type == \"audio\"), None)\n",
    "    if audio_stream is None:\n",
    "        raise RuntimeError(\"No audio stream found in container\")\n",
    "\n",
    "    # Define sample rate ONCE, from the stream (robust + simple)\n",
    "    if getattr(audio_stream, \"rate\", None) is None:\n",
    "        raise RuntimeError(\"Audio stream has no sample rate (audio_stream.rate is None)\")\n",
    "    sr = int(audio_stream.rate)\n",
    "\n",
    "    audio_frames_lr: list[np.ndarray] = []\n",
    "\n",
    "    for audio_frame in container.decode(audio_stream):\n",
    "        arr = audio_frame.to_ndarray()\n",
    "\n",
    "        # Normalize to shape (channels, samples)\n",
    "        if arr.ndim == 1:\n",
    "            # mono interleaved -> (1, samples)\n",
    "            arr = arr[np.newaxis, :]\n",
    "        elif arr.ndim == 2:\n",
    "            # could already be (channels, samples) (planar)\n",
    "            # if it's (samples, channels) swap it\n",
    "            if arr.shape[0] > arr.shape[1]:\n",
    "                # heuristic: more samples than channels -> (samples, channels)\n",
    "                # so transpose to (channels, samples)\n",
    "                arr = arr.T\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unexpected audio ndarray shape: {arr.shape}\")\n",
    "\n",
    "        audio_frames_lr.append(arr.astype(np.float32, copy=False))\n",
    "\n",
    "    audio_lr = np.concatenate(audio_frames_lr, axis=1) if audio_frames_lr else np.zeros((1, 0), dtype=np.float32)\n",
    "    channels = audio_lr.shape[0]\n",
    "\n",
    "\n",
    "    # Initialize plot\n",
    "    font_size = 10\n",
    "    font_path = Path(\"~/.local/share/fonts/google/roboto_mono/RobotoMono-VariableFont_wght.ttf\").expanduser()\n",
    "    mono_fp = fm.FontProperties(fname=font_path, size=font_size)\n",
    "\n",
    "    xmin = 10 ** (FLOOR_DB / 20.0)  # amplitude at -60 dBFS\n",
    "    xmax = 10 ** (CEIL_DB / 20.0)   # amplitude at 0 dBFS = 1.0\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(2.7, 0.8), dpi=150)\n",
    "\n",
    "    bars = ax.barh(\n",
    "        [\"CH2\", \"CH1\"],\n",
    "        [0.0, 0.0],\n",
    "        left=xmin,\n",
    "        color=\"white\",\n",
    "        edgecolor=\"white\",\n",
    "        alpha=1.0,\n",
    "        height=0.6,\n",
    "    )\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    # ax.set_xlabel(\"Amplitude\", fontproperties=mono_fp)\n",
    "    # ax.set_title(\"Loudness\", fontproperties=mono_fp)\n",
    "\n",
    "    # dB tick labels on the X axis\n",
    "    # [-60, -56, -52, -48, -44, -40, -36, -32, -28, -24, -20, -16, -12, -8, -4, 0]\n",
    "    # [-60, -48, -36, -24, -12, -6, 0]\n",
    "    ticks_db = np.array([-36, -12, 0], dtype=float)\n",
    "    ax.set_xticks(10 ** (ticks_db / 20.0))\n",
    "    ax.set_xticklabels([f\"{int(t)} dB\" for t in ticks_db])\n",
    "\n",
    "    ax.grid(True, which=\"both\", axis=\"x\", alpha=0.3)\n",
    "\n",
    "    txt = fig.text(\n",
    "        0.2, 0.98, \"\",\n",
    "        ha=\"left\", va=\"top\",\n",
    "        color=\"white\",\n",
    "        fontproperties=mono_fp,\n",
    "    )\n",
    "\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        label.set_fontproperties(mono_fp)\n",
    "\n",
    "    fig.subplots_adjust(top=1.20)   # leaves room for the text above the axes\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "    while True:\n",
    "        ok, frame_bgr = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        t0 = frame_idx * frame_duration\n",
    "        t1 = (frame_idx + 1) * frame_duration\n",
    "\n",
    "        i0 = int(t0 * sr)\n",
    "        i1 = int(t1 * sr)\n",
    "\n",
    "        # l_dbfs, r_dbfs = frame_loudness_lr(audio_lr, i0, i1)\n",
    "        l_dbfs, r_dbfs = frame_volume_lr_peak(audio_lr, i0, i1)\n",
    "\n",
    "\n",
    "        # Plot loudness meter\n",
    "        l_val = float(l_dbfs) if (l_dbfs is not None and np.isfinite(l_dbfs)) else FLOOR_DB\n",
    "        r_val = float(r_dbfs) if (r_dbfs is not None and np.isfinite(r_dbfs)) else np.nan\n",
    "\n",
    "        l_plot_db = float(np.clip(l_val, FLOOR_DB, CEIL_DB))\n",
    "        r_plot_db = FLOOR_DB if not np.isfinite(r_val) else float(np.clip(r_val, FLOOR_DB, CEIL_DB))\n",
    "\n",
    "        l_amp = 10 ** (l_plot_db / 20.0)\n",
    "        r_amp = 10 ** (r_plot_db / 20.0)\n",
    "\n",
    "        # Baseline stays at xmin (=-60 dBFS in amplitude)\n",
    "        bars[0].set_x(xmin)\n",
    "        bars[1].set_x(xmin)\n",
    "\n",
    "        # Width is offset from baseline (amplitude units)\n",
    "        bars[0].set_width(r_amp - xmin)\n",
    "        bars[1].set_width(l_amp - xmin)\n",
    "\n",
    "        # bars[1].set_alpha(0.25 if not np.isfinite(r_val) else 1.0)\n",
    "\n",
    "        txt.set_text(f\"{l_plot_db:5.1f} dB       {'â€”' if not np.isfinite(r_val) else f'{r_plot_db:5.1f} dB'}\")\n",
    "        # txt.set_text(f\"{r_plot_db:5.1f} dB       {r_plot_db:5.1f} dB\")\n",
    "\n",
    "        out_png = path.join(AUDIOMETER_OUTDIR, f\"{frame_idx:07d}.png\")\n",
    "        fig.savefig(out_png, format=\"png\", transparent=True)\n",
    "\n",
    "\n",
    "        frame_idx += 1\n",
    "        plt.close(fig)\n",
    "\n",
    "    cap.release()\n",
    "    container.close()"
   ],
   "id": "96af825d02b8d2c3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
